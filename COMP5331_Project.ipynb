{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVPzFZH4gE_B"
   },
   "source": [
    "# COMP5331 Group 6 Project: Resilient k-Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yw95uRdmfuqc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import random "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(point1, point2):\n",
    "    # This function is to calculate the distance between 2 points\n",
    "    distance_x_y = np.linalg.norm(np.array(point1) - np.array(point2))\n",
    "    return distance_x_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://dev.to/theramoliya/python-kruskals-algorithm-for-minimum-spanning-trees-2bmb\n",
    "class Graph:\n",
    "    def __init__(self, vertices, graph=None):\n",
    "        self.V = vertices\n",
    "        if graph is None:\n",
    "            self.graph = []\n",
    "        else:\n",
    "            self.graph = copy.deepcopy(graph)\n",
    "        \n",
    "    def copy(self):\n",
    "        return Graph(self.V, self.graph)\n",
    "\n",
    "    def add_edge(self, u, v, w):\n",
    "        self.graph.append([u, v, w])\n",
    "    \n",
    "    def remove_edge(self, u, v, w):\n",
    "        self.graph.remove([u, v, w])\n",
    "\n",
    "    def kruskal_mst(self):\n",
    "        def find(parent, i):\n",
    "            if parent[i] == i:\n",
    "                return i\n",
    "            return find(parent, parent[i])\n",
    "\n",
    "        def union(parent, rank, x, y):\n",
    "            x_root = find(parent, x)\n",
    "            y_root = find(parent, y)\n",
    "\n",
    "            if rank[x_root] < rank[y_root]:\n",
    "                parent[x_root] = y_root\n",
    "            elif rank[x_root] > rank[y_root]:\n",
    "                parent[y_root] = x_root\n",
    "            else:\n",
    "                parent[y_root] = x_root\n",
    "                rank[x_root] += 1\n",
    "\n",
    "        result = []\n",
    "        i = 0\n",
    "        e = 0\n",
    "\n",
    "        self.graph = sorted(self.graph, key=lambda item: item[2])\n",
    "        parent = [i for i in range(self.V)]\n",
    "        rank = [0] * self.V\n",
    "\n",
    "        while e < self.V - 1:\n",
    "            u, v, w = self.graph[i]\n",
    "            i += 1\n",
    "            x = find(parent, u)\n",
    "            y = find(parent, v)\n",
    "\n",
    "            if x != y:\n",
    "                e += 1\n",
    "                result.append([u, v, w])\n",
    "                union(parent, rank, x, y)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Approx_algo:\n",
    "    def __init__(self, dataset, k, seed=5331):\n",
    "        self.dataset = dataset\n",
    "        self.k = k\n",
    "        self.seed = seed\n",
    "\n",
    "    # Define KCluster\n",
    "    class Cluster:\n",
    "        def __init__(self):\n",
    "            self.elements = [] # Initially, cluster has no points inside\n",
    "            self.head = None \n",
    "            \n",
    "    def clustering(self):\n",
    "        def initialize_clusters(dataset, seed = None):\n",
    "            # Since we need to initialize the first cluster, there must be someone who does that first\n",
    "            cluster = Approx_algo.Cluster() # call class Cluster\n",
    "            cluster.elements = dataset.tolist() # All data now become the point of cluster 1\n",
    "            if seed is not None:\n",
    "                random.seed(seed)\n",
    "            cluster.head = random.choice(cluster.elements)\n",
    "            return [cluster]\n",
    "        \n",
    "        def expand_clusters(clusters, j):\n",
    "            # At this function, we will perform expansion\n",
    "\n",
    "            # We will find the point with maximal distance to the head\n",
    "            max_distance = -1\n",
    "            v_i = None \n",
    "\n",
    "            for i in range(j - 1):\n",
    "                current_cluster = clusters[i]\n",
    "\n",
    "                for point in current_cluster.elements:\n",
    "                    dist = distance(point, current_cluster.head)\n",
    "                    if dist > max_distance:\n",
    "                        max_distance = dist\n",
    "                        v_i = point\n",
    "            \n",
    "            # Create new cluster B_(j + 1)\n",
    "            new_cluster = Approx_algo.Cluster()\n",
    "            new_cluster.head = v_i \n",
    "            new_cluster.elements = []\n",
    "            \n",
    "            # Move elements to the new cluster\n",
    "            for i in range(j - 1):\n",
    "                current_cluster = clusters[i]\n",
    "                # print(\"head \", i+1, current_cluster.head)\n",
    "                for point in current_cluster.elements:\n",
    "                    # print(\"distance v_1\", distance(point, v_i))\n",
    "                    # print(\"distance head\", distance(point, current_cluster.head))\n",
    "                    if distance(point, v_i) <= distance(point, current_cluster.head):\n",
    "                        # print(\"point \", point)\n",
    "                        new_cluster.elements.append(point)\n",
    "                \n",
    "                # Delete the elements that was appended to new cluster\n",
    "                current_cluster.elements = [element for element in current_cluster.elements if element not in new_cluster.elements]\n",
    "                # print(\"Cluster \", i + 1, \" : \", current_cluster.elements)\n",
    "\n",
    "            # Add this new cluster to a list of cluster\n",
    "            clusters.append(new_cluster)\n",
    "\n",
    "            return clusters\n",
    "    \n",
    "        def get_heads(clusters):\n",
    "            # Give me the list of current clusters head\n",
    "            heads = []\n",
    "            for cluster in clusters:\n",
    "                heads.append(cluster.head)\n",
    "            \n",
    "            return heads\n",
    "\n",
    "        clusters = initialize_clusters(self.dataset, self.seed)\n",
    "        for self.k in range(2, self.k + 1): # note that it should be range(2, k+1), we start from 2 because we already initialize a cluster\n",
    "            clusters = expand_clusters(clusters, self.k)\n",
    "\n",
    "        # Get the heads of the clusters\n",
    "        heads = get_heads(clusters)\n",
    "\n",
    "        return heads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "j1bo0eIXrOj4"
   },
   "outputs": [],
   "source": [
    "class resilient_k_center():\n",
    "    def __init__(self, dataset, k, epsilon, lamb=0.1, seed=5331):\n",
    "        self.dataset = dataset\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.lamb = lamb\n",
    "        self.number_of_centers = int(2 * self.k * np.log(1 / self.epsilon))\n",
    "        self.seed = seed\n",
    "\n",
    "    def resilient_k_center(self):\n",
    "        # randomly assign centers (line 1)\n",
    "        centers = self.dataset[np.random.choice(self.dataset.shape[0],\n",
    "                                                self.number_of_centers,\n",
    "                                                replace=False)]\n",
    "        print(\"Centers: \\n:\", centers)\n",
    "\n",
    "        # construct edges and weights (line 2-4)\n",
    "        E = []\n",
    "        w = {}\n",
    "        for index_p, p in enumerate(self.dataset):\n",
    "            for index_q, q in enumerate(self.dataset):\n",
    "                if index_p < index_q:\n",
    "                    if (p.tolist() in centers.tolist()) or (q.tolist() in centers.tolist()):\n",
    "                        E.append((index_p, index_q))\n",
    "\n",
    "                        # Integrated line 1-7 of algorithm Resilient-MST in the weight update below\n",
    "                        if (p.tolist() in centers.tolist()) and (q.tolist() in centers.tolist()):\n",
    "                            w[(index_p, index_q)] = 0\n",
    "                        else:\n",
    "                            alpha = np.random.rand()\n",
    "                            i = math.ceil(alpha + math.log(distance(p, q), self.lamb))\n",
    "                            w[(index_p, index_q)] = self.lamb ** (i - alpha)\n",
    "        print(E)\n",
    "        print(w)\n",
    "        \n",
    "        # construct weighted graph (line 5)\n",
    "        g = Graph(len(self.dataset))\n",
    "        for edge in E:\n",
    "            index_p, index_q = edge\n",
    "            g.add_edge(index_p, index_q, w[(index_p, index_q)])\n",
    "            g.add_edge(index_q, index_p, w[(index_p, index_q)])\n",
    "        print(\"weighted graph: \\n\", g.graph)    \n",
    "\n",
    "        # construct MST (line 6)\n",
    "        T = g.kruskal_mst()\n",
    "        print(\"resilient MST: \\n\", T)\n",
    "\n",
    "        # assign clusters (line 7-8)\n",
    "        cluster = []\n",
    "        for index_p, p in enumerate(self.dataset):\n",
    "            if p.tolist() in centers.tolist():\n",
    "                cluster.append((p, p))\n",
    "            else:\n",
    "                for edge in T:\n",
    "                    index_u, index_v, _ = edge\n",
    "                    if (index_p == index_u) and (self.dataset[index_v].tolist() in centers.tolist()):\n",
    "                        cluster.append((p, self.dataset[index_v]))\n",
    "                    elif (index_p == index_v) and (self.dataset[index_u].tolist() in centers.tolist()):\n",
    "                        cluster.append((p, self.dataset[index_u]))\n",
    "        \n",
    "        print(\"Initial Cluster: \\n\", cluster)\n",
    "\n",
    "        # find non-center vertices which incident to the heaviest edges (line 9)\n",
    "        n = len(self.dataset)\n",
    "        heaviest_edges = sorted(T, key=lambda item: item[2], reverse=True)[:int(self.epsilon * n)]\n",
    "        print(\"Heaviest Edges: \\n\", heaviest_edges)\n",
    "        L = set()   \n",
    "        for edge in heaviest_edges:\n",
    "            index_p, index_q, _ = edge\n",
    "            if (self.dataset[index_p].tolist() not in centers.tolist()):\n",
    "                L.add(index_p)\n",
    "            if (self.dataset[index_q].tolist() not in centers.tolist()):\n",
    "                L.add(index_q)\n",
    "        print(\"L: \\n\", L)\n",
    "\n",
    "        # centres selected by Algorithm Approx [27] (line 10)\n",
    "        approx_algo = Approx_algo(self.dataset, self.k, self.seed)\n",
    "        centers_approx = approx_algo.clustering()\n",
    "        print(\"Centers selected by Approx: \\n\", centers_approx)\n",
    "\n",
    "        # Update the center as the closest center of C' to each point in L (line 11)\n",
    "        for index_p in L:\n",
    "            min_dist = float('inf')\n",
    "            closest_center = None\n",
    "            for c in centers_approx:\n",
    "                dist = distance(self.dataset[index_p], c)\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    closest_center = c\n",
    "            if (closest_center is not None):\n",
    "                for index_c, cluster_pair in enumerate(cluster):\n",
    "                    p, _ = cluster_pair\n",
    "                    if np.array_equal(p, self.dataset[index_p]):\n",
    "                        cluster[index_c] = (p, closest_center)\n",
    "                        break\n",
    "                    \n",
    "        print(\"Final Cluster: \\n\", cluster)\n",
    "        return cluster\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1p2F0xu0re6q",
    "outputId": "1e9ad1cf-9e94-49a2-8f06-b3189d88d28b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centers: \n",
      ": [[ 7  8]\n",
      " [ 9 10]]\n",
      "[(0, 3), (0, 4), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n",
      "{(0, 3): 8.394079400821992, (0, 4): 1.134207058459542, (1, 3): 3.002091871985522, (1, 4): 2.165705309197452, (2, 3): 1.0695333251871488, (2, 4): 0.655603679481183, (3, 4): 0}\n",
      "weighted graph: \n",
      " [[0, 3, 8.394079400821992], [3, 0, 8.394079400821992], [0, 4, 1.134207058459542], [4, 0, 1.134207058459542], [1, 3, 3.002091871985522], [3, 1, 3.002091871985522], [1, 4, 2.165705309197452], [4, 1, 2.165705309197452], [2, 3, 1.0695333251871488], [3, 2, 1.0695333251871488], [2, 4, 0.655603679481183], [4, 2, 0.655603679481183], [3, 4, 0], [4, 3, 0]]\n",
      "resilient MST: \n",
      " [[3, 4, 0], [2, 4, 0.655603679481183], [0, 4, 1.134207058459542], [1, 4, 2.165705309197452]]\n",
      "Initial Cluster: \n",
      " [(array([1, 2]), array([ 9, 10])), (array([3, 4]), array([ 9, 10])), (array([5, 6]), array([ 9, 10])), (array([7, 8]), array([7, 8])), (array([ 9, 10]), array([ 9, 10]))]\n",
      "Heaviest Edges: \n",
      " [[1, 4, 2.165705309197452]]\n",
      "L: \n",
      " {1}\n",
      "Centers selected by Approx: \n",
      " [[9, 10]]\n",
      "Final Cluster: \n",
      " [(array([1, 2]), array([ 9, 10])), (array([3, 4]), [9, 10]), (array([5, 6]), array([ 9, 10])), (array([7, 8]), array([7, 8])), (array([ 9, 10]), array([ 9, 10]))]\n"
     ]
    }
   ],
   "source": [
    "test = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
    "k = 1\n",
    "epilson = 0.3\n",
    "model = resilient_k_center(test, k, epilson)\n",
    "cluster = model.resilient_k_center()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centers: \n",
      ": [[ 0  4]\n",
      " [12  4]]\n",
      "[(0, 2), (0, 3), (1, 2), (1, 3), (2, 3), (2, 4), (2, 5), (3, 4), (3, 5)]\n",
      "{(0, 2): 0.41951288296193295, (0, 3): 2.670972829764518, (1, 2): 0.8491531708434129, (1, 3): 5.281557868857898, (2, 3): 0, (2, 4): 4.320047157428978, (2, 5): 3.1828420196047422, (3, 4): 2.9926182372809524, (3, 5): 4.1379806426038925}\n",
      "weighted graph: \n",
      " [[0, 2, 0.41951288296193295], [2, 0, 0.41951288296193295], [0, 3, 2.670972829764518], [3, 0, 2.670972829764518], [1, 2, 0.8491531708434129], [2, 1, 0.8491531708434129], [1, 3, 5.281557868857898], [3, 1, 5.281557868857898], [2, 3, 0], [3, 2, 0], [2, 4, 4.320047157428978], [4, 2, 4.320047157428978], [2, 5, 3.1828420196047422], [5, 2, 3.1828420196047422], [3, 4, 2.9926182372809524], [4, 3, 2.9926182372809524], [3, 5, 4.1379806426038925], [5, 3, 4.1379806426038925]]\n",
      "resilient MST: \n",
      " [[2, 3, 0], [0, 2, 0.41951288296193295], [1, 2, 0.8491531708434129], [3, 4, 2.9926182372809524], [2, 5, 3.1828420196047422]]\n",
      "Initial Cluster: \n",
      " [(array([0, 8]), array([0, 4])), (array([4, 8]), array([0, 4])), (array([0, 4]), array([0, 4])), (array([12,  4]), array([12,  4])), (array([12,  0]), array([12,  4])), (array([8, 0]), array([0, 4]))]\n",
      "Heaviest Edges: \n",
      " [[2, 5, 3.1828420196047422]]\n",
      "L: \n",
      " {5}\n",
      "Centers selected by Approx: \n",
      " [[12, 0]]\n",
      "Final Cluster: \n",
      " [(array([0, 8]), array([0, 4])), (array([4, 8]), array([0, 4])), (array([0, 4]), array([0, 4])), (array([12,  4]), array([12,  4])), (array([12,  0]), array([12,  4])), (array([8, 0]), [12, 0])]\n"
     ]
    }
   ],
   "source": [
    "test2 = np.array([[0, 8], [4, 8], [0, 4], [12, 4], [12, 0], [8, 0]])\n",
    "k2 = 1\n",
    "epilson2 = 0.3\n",
    "model2 = resilient_k_center(test2, k2, epilson2)\n",
    "cluster2 = model2.resilient_k_center()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvIukeIOG2Ad"
   },
   "source": [
    "## Evalutaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gonzalez implementation from https://github.com/TSunny007/Clustering/blob/master/notebooks/Gonzalez.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://github.com/TSunny007/Clustering/blob/master/notebooks/Gonzalez.ipynb\n",
    "from scipy.spatial import distance\n",
    "def max_dist(data, clusters):\n",
    "    distances = np.zeros(len(data)) # we will keep a cumulative distance measure for all points\n",
    "    for cluster_id, cluster in enumerate(clusters):\n",
    "        for point_id, point in enumerate(data):\n",
    "            if distance.euclidean(point,cluster) == 0.0:\n",
    "                distances[point_id] = -math.inf # this point is already a cluster \n",
    "            if not math.isinf(distances[point_id]):\n",
    "                distances[point_id] = distances[point_id] + distance.euclidean(point,cluster) \n",
    "    return data[np.argmax(distances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_dist(data, clusters):\n",
    "    distances = np.zeros(len(data)) # we will keep a cumulative distance measure for all points\n",
    "    for point_id, point in enumerate(data):\n",
    "        for cluster_id, cluster in enumerate(clusters):\n",
    "            if distance.euclidean(point,cluster) == 0.0:\n",
    "                distances[point_id] = -math.inf # this point is already a cluster (obselete)\n",
    "            if not math.isinf(distances[point_id]):\n",
    "                # if a point is not obselete, then we add the distance to its specific bin\n",
    "                distances[point_id] = distances[point_id] + math.pow(distance.euclidean(point,cluster),2) \n",
    "                # return the point which is furthest away from all the other clusters\n",
    "    for distance_id, current_distance in enumerate(distances):\n",
    "        if not math.isinf(current_distance): \n",
    "            distances[distance_id] = math.sqrt(current_distance/len(data))\n",
    "    return data[np.argmax(distances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuCZ9yIBG1gq"
   },
   "outputs": [],
   "source": [
    "def gonzalez(data, cluster_num, method = 'max'):\n",
    "    clusters = []\n",
    "    clusters.append(data[0]) # assign the first point to the first cluster\n",
    "    while len(clusters) < cluster_num:\n",
    "        if method is 'max':\n",
    "            clusters.append(max_dist(data, clusters)) \n",
    "        if method is 'norm':\n",
    "            clusters.append(norm_dist(data, clusters)) \n",
    "        # we add the furthest point from ALL current clusters\n",
    "    return (clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carving algorithm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def distance(point1, point2):\n",
    "    return np.linalg.norm(np.array(point1) - np.array(point2))\n",
    "\n",
    "def carve(points, R, k):\n",
    "    centers = []\n",
    "    uncovered_indices = set(range(len(points)))  # Indices of uncovered points\n",
    "\n",
    "    while uncovered_indices and len(centers) < k:\n",
    "        # Randomly select an uncovered point\n",
    "        idx = random.choice(list(uncovered_indices))\n",
    "        center = points[idx]\n",
    "        centers.append(center)\n",
    "\n",
    "        # Mark all points within distance R from the new center as covered\n",
    "        to_remove = []\n",
    "        for i in uncovered_indices:\n",
    "            if distance(center, points[i]) <= R:\n",
    "                to_remove.append(i)\n",
    "\n",
    "        # Remove covered points from uncovered set\n",
    "        uncovered_indices.difference_update(to_remove)\n",
    "\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_minimum_R(points, k, R_start, R_end, step=0.1):\n",
    "    best_R = None\n",
    "\n",
    "    R = R_start\n",
    "    while R <= R_end:\n",
    "        centers = carve(points, R, k)\n",
    "        if len(centers) <= k:  # Check if we opened at most k centers\n",
    "            best_R = R  # Update best R found\n",
    "            R -= step  # Try a smaller R\n",
    "        else:\n",
    "            R += step  # Increase R\n",
    "    return best_R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Medoid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def distance_matrix(points):\n",
    "    return np.linalg.norm(points[:, np.newaxis] - points, axis=2)\n",
    "\n",
    "def assign_clusters(points, medoids):\n",
    "    distances = distance_matrix(points)\n",
    "    cluster_assignment = np.argmin(distances[:, medoids], axis=1)\n",
    "    return cluster_assignment\n",
    "\n",
    "def update_medoids(points, clusters, k):\n",
    "    new_medoids = []\n",
    "    for i in range(k):\n",
    "        # Get points in the current cluster\n",
    "        cluster_points = points[clusters == i]\n",
    "        if len(cluster_points) == 0:\n",
    "            continue\n",
    "\n",
    "        # Calculate the cost for each point in the cluster as a potential medoid\n",
    "        costs = []\n",
    "        for point in cluster_points:\n",
    "            cost = np.sum(np.linalg.norm(cluster_points - point, axis=1))\n",
    "            costs.append(cost)\n",
    "\n",
    "        # Find the point with the minimum cost\n",
    "        new_medoid = cluster_points[np.argmin(costs)]\n",
    "        new_medoids.append(np.where((points == new_medoid).all(axis=1))[0][0])\n",
    "\n",
    "    return np.array(new_medoids)\n",
    "\n",
    "def pam(points, k, max_iter=100):\n",
    "    # Randomly select k initial medoids\n",
    "    initial_indices = np.random.choice(len(points), k, replace=False)\n",
    "    medoids = initial_indices.copy()\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Step 1: Assign clusters based on current medoids\n",
    "        clusters = assign_clusters(points, medoids)\n",
    "\n",
    "        # Step 2: Update medoids\n",
    "        new_medoids = update_medoids(points, clusters, k)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.array_equal(medoids, new_medoids):\n",
    "            break\n",
    "\n",
    "        medoids = new_medoids\n",
    "\n",
    "    return medoids, clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraction_points_changing_cluster(old_clusters, new_clusters):\n",
    "    changes = np.sum(old_clusters != new_clusters)\n",
    "    total_points = len(old_clusters)\n",
    "    return changes / total_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution_cost(points, clusters, medoids):\n",
    "    max_distance = 0\n",
    "    for i, point in enumerate(points):\n",
    "        medoid = medoids[clusters[i]]\n",
    "        distance = np.linalg.norm(point - points[medoid])\n",
    "        max_distance = max(max_distance, distance)\n",
    "    return max_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_clusters(clusters):\n",
    "    \"\"\"Count the number of unique clusters formed.\"\"\"\n",
    "    return len(np.unique(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
