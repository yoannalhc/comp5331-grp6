{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVPzFZH4gE_B"
   },
   "source": [
    "# COMP5331 Group 6 Project: Resilient k-Clustering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess datasets\n",
    "The raw data should be stored at:\n",
    "```\n",
    "project\n",
    "ðŸ“‚dataset\n",
    "â””â”€â”€â”€ðŸ“‚birch\n",
    "â”‚       â”‚ ðŸ“œbirch1.txt\n",
    "â”‚       â”‚ ðŸ“œbirch2.txt\n",
    "â”‚       â”‚ ðŸ“œbirch3.txt\n",
    "â””â”€â”€â”€ðŸ“‚high_dim\n",
    "â”‚       â”‚ ðŸ“œdim032.txt\n",
    "â”‚       â”‚ ðŸ“œdim064.txt\n",
    "â”‚       â”‚ ðŸ“œdim128.txt\n",
    "â””â”€â”€â”€ðŸ“‚snap_standford\n",
    "â”‚       â”‚ ðŸ“œBrightkite_totalCheckins.txt\n",
    "â”‚       â”‚ ðŸ“œGowalla_totalCheckins.txt\n",
    "â””â”€â”€â”€ðŸ“‚uber\n",
    "        â”‚ ðŸ“œuber-raw-data-jun14.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.preprocess.process_uber import process_uber\n",
    "ds_path = \"./dataset/uber/uber-raw-data-jun14.csv\"\n",
    "save_path = \"./dataset/uber/\"\n",
    "process_uber(ds_path, save_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.preprocess.process_geo import process_geo\n",
    "ds_names = [\"Brightkite\", \"Gowalla\"]\n",
    "for ds_name in ds_names:\n",
    "    print(f\"Processing {ds_name} dataset\")\n",
    "    ds_path = f\"./dataset/snap_standford/{ds_name}_totalCheckins.txt\"\n",
    "    save_path = \"./dataset/snap_standford/\"\n",
    "    process_geo(ds_path, save_path, ds_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.preprocess.process_birch import process_birch, shrink_birch\n",
    "\n",
    "ds_names = ['birch1', 'birch2', 'birch3']\n",
    "\n",
    "for ds_name in ds_names:\n",
    "    ds_path = f\"./dataset/birch/{ds_name}.txt\"\n",
    "    save_path = \"./dataset/birch\"\n",
    "    process_birch(ds_path, save_path, ds_name)\n",
    "    ds_path = f\"./dataset/birch/{ds_name}_epsilon.csv\"\n",
    "    shrink_birch(ds_path, save_path, ds_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.preprocess.process_high_dim import process_high_dim\n",
    "dims = ['032', '064', '128']\n",
    "\n",
    "for dim in dims:\n",
    "    ds_name = \"dim\"+dim\n",
    "    ds_path = f\"./dataset/high_dim/{ds_name}.txt\"\n",
    "    save_path = \"./dataset/high_dim\"\n",
    "    process_high_dim(ds_path, save_path, ds_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.plot_helper import plot_data\n",
    "from src.datasets import *\n",
    "from os.path import join, isdir\n",
    "from os import mkdir\n",
    "ds_path = \"./dataset\"\n",
    "dataset = [Uber(join(ds_path, \"uber/uber_epsilon.csv\"), lamb=1.1, k=10), \n",
    "           Geo(join(ds_path, \"snap_standford/Brightkite_epsilon.csv\"), \"Brightkite\", lamb=1.1, k=50),\n",
    "           Geo(join(ds_path, \"snap_standford/Gowalla_epsilon.csv\"), \"Gowalla\", lamb=1.001, k=50),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch1_epsilon.csv\"), subset=1, lamb=1.1, k=10),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch2_epsilon.csv\"), subset=2, lamb=1.1, k=10),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch3_epsilon.csv\"), subset=3, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim032_epsilon.csv\"), dim=32, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim064_epsilon.csv\"), dim=64, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim128_epsilon.csv\"), dim=128, lamb=1.1, k=10)\n",
    "           ]\n",
    "plot_path = \"./dataset/plot\"\n",
    "if not isdir(plot_path):\n",
    "    mkdir(plot_path)\n",
    "for ds in dataset:\n",
    "    pair1, pair2 = ds.load()\n",
    "    plot_data(pair1, pair2, plot_path, ds.name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate possible range for epsilon"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pprint import pprint\n",
    "def pick_epsilon(lambda_, n):\n",
    "    epsilion_lower_bound = (3 * np.log(ds.lamb) * np.log(n) ** 2) / n\n",
    "    epsilion_upper_bound = 1 \n",
    "    epsilon = np.random.uniform(epsilion_lower_bound, epsilion_upper_bound)\n",
    "    while (1+epsilon) >= lambda_:\n",
    "        epsilon = np.random.uniform(epsilion_lower_bound, epsilion_upper_bound)\n",
    "    \n",
    "    return epsilon\n",
    "dataset = [Uber(join(ds_path, \"uber/uber_epsilon.csv\"), lamb=1.1, k=10), \n",
    "           Geo(join(ds_path, \"snap_standford/Brightkite_epsilon.csv\"), \"Brightkite\", lamb=1.1, k=50),\n",
    "           Geo(join(ds_path, \"snap_standford/Gowalla_epsilon.csv\"), \"Gowalla\", lamb=1.001, k=50),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch1_epsilon.csv\"), subset=1, lamb=1.1, k=10),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch2_epsilon.csv\"), subset=2, lamb=1.1, k=10),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch3_epsilon.csv\"), subset=3, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim032_epsilon.csv\"), dim=32, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim064_epsilon.csv\"), dim=64, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim128_epsilon.csv\"), dim=128, lamb=1.1, k=10)\n",
    "           ]\n",
    "# Create epsilon for each lambda and data size combination\n",
    "\n",
    "eps_dict = {}\n",
    "for ds in dataset:\n",
    "    lamb = ds.lamb\n",
    "    pair1, pair2 = ds.load()\n",
    "    n = len(pair1)\n",
    "    print(f'{lamb}, {n}, {pick_epsilon(lamb, n)}')\n",
    "    if lamb == 1.1:\n",
    "        epsilon = 0.05\n",
    "    elif lamb == 1.001:\n",
    "        epsilon = 0.0005\n",
    "    eps_dict[(lamb,n)] = epsilon\n",
    "pprint(eps_dict)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# alter these 2 variable to generate different result \n",
    "basline_set_random_seed = False\n",
    "resilient_set_random_seed = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-resillient"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yw95uRdmfuqc"
   },
   "source": [
    "import numpy as np\n",
    "from src.resilient_k import resilient_k_center\n",
    "from src.datasets import *\n",
    "from os.path import join, isdir\n",
    "from os import mkdir, makedirs\n",
    "from src.plot_helper import plot_cluster_result\n",
    "import pickle"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def experiment(ds, resilient_k_param, plot_path, result_path, epsilon):\n",
    "    for algo in resilient_k_param[\"algorithm\"]:\n",
    "        for alpha in resilient_k_param[\"alpha\"]:\n",
    "            for beta in resilient_k_param[\"beta\"]:  \n",
    "                if resilient_set_random_seed:\n",
    "                    loop_content = resilient_k_param[\"seed\"]\n",
    "                else:\n",
    "                    loop_content = [None] * len(resilient_k_param[\"seed\"])\n",
    "                for i, seed in enumerate(loop_content):   \n",
    "                    \n",
    "                    print(f\"Processing {ds.name} dataset with k={ds.k}, lamb={ds.lamb}, alpha={alpha}, beta={beta}, algorithm={algo}, seed={seed}, epsilon={epsilon}\")\n",
    "                    pair1, pair2 = ds.load()                    \n",
    "                    \n",
    "                    model1 = resilient_k_center(pair1, k=ds.k, lamb=ds.lamb, epsilon=epsilon, alpha=alpha, beta=beta, algorithm=algo, seed=seed)\n",
    "                    #start_time = time.process_time_ns()\n",
    "                    center1, cluster1 = model1.resilient_k_center()\n",
    "                    #end_time = time.process_time_ns()\n",
    "                    #time_taken1 = end_time - start_time\n",
    "                    \n",
    "                    model2 = resilient_k_center(pair2, k=ds.k, lamb=ds.lamb, epsilon=epsilon, alpha=alpha, beta=beta, algorithm=algo, seed=seed)\n",
    "                    #start_time = time.process_time_ns()\n",
    "                    center2, cluster2 = model2.resilient_k_center()\n",
    "        \n",
    "                    #end_time = time.process_time_ns()\n",
    "                    #time_taken2 = end_time - start_time\n",
    "                    if seed == None:\n",
    "                        seed = \"None\" + \"_\" + str(i)\n",
    "                    seed_result_path = join(result_path, f'{seed}')\n",
    "                    if not isdir(seed_result_path):\n",
    "                        mkdir(seed_result_path)\n",
    "                    this_result_path = join(seed_result_path, f'{ds.name}')\n",
    "                    if not isdir(this_result_path):\n",
    "                        mkdir(this_result_path)\n",
    "                        \n",
    "                    with open(join(this_result_path, f\"{ds.name}_resilient_{ds.k}_{algo}({alpha}_{beta}).pickle\"), 'wb') as output_file:\n",
    "                        #pickle.dump((center1, cluster1, center2, cluster2, time_taken1, time_taken2), output_file)\n",
    "                        pickle.dump((center1, cluster1, center2, cluster2), output_file)\n",
    "                        \n",
    "                    pt1 = np.asarray([c[0] for c in cluster1])\n",
    "                    pt2 = np.asarray([c[0] for c in cluster2])\n",
    "                    label1 = [c[1] for c in cluster1]\n",
    "                    label2 = [c[1] for c in cluster2]\n",
    "                    \n",
    "                    # for plotting labelled clustering result\n",
    "                    seed_plot_path = join(plot_path, f'{seed}')\n",
    "                    if not isdir(seed_plot_path):\n",
    "                        mkdir(seed_plot_path)\n",
    "                    this_plot_path = join(seed_plot_path, ds.name)\n",
    "                    if not isdir(this_plot_path):\n",
    "                        mkdir(this_plot_path)\n",
    "                        \n",
    "                    plot_cluster_result(pt1, pt2, label1, label2, this_plot_path, ds, f'Con{algo[0].title()}({alpha}, {beta})')\n",
    "                    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ds_path = \"./dataset\"\n",
    "resilient_k_param ={\"alpha\": [0.5, 1.0], \n",
    "                    \"beta\": [0.5, 1.0],\n",
    "                    \"algorithm\": [\"gonz\", \"carv\"],\n",
    "                    \"seed\": [5331,5332,5333]}\n",
    "plot_path = \"./results/plot/resilient_k/\"\n",
    "result_path = \"./results/resilient_k/\"\n",
    "if not isdir(plot_path):\n",
    "    makedirs(plot_path)\n",
    "if not isdir(result_path):\n",
    "    makedirs(result_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Uber"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1p2F0xu0re6q",
    "outputId": "1e9ad1cf-9e94-49a2-8f06-b3189d88d28b"
   },
   "source": [
    "dataset = [Uber(join(ds_path, \"uber/uber_epsilon.csv\"), lamb=1.1, k=10),\n",
    "           Uber(join(ds_path, \"uber/uber_epsilon.csv\"), lamb=1.1, k=20)]\n",
    "\n",
    "for ds in dataset:\n",
    "    experiment(ds, resilient_k_param, plot_path, result_path, epsilon=eps_dict[(ds.lamb, len(pair1))])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Brightkite"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset = [Geo(join(ds_path, \"snap_standford/Brightkite_epsilon.csv\"), \"Brightkite\", lamb=1.1, k=50), \n",
    "           Geo(join(ds_path, \"snap_standford/Brightkite_epsilon.csv\"), \"Brightkite\", lamb=1.1, k=100)]#,\n",
    "           #Geo(join(ds_path, \"snap_standford/Gowalla_epsilon.csv\"), \"Gowalla\", lamb=1.001, k=50),\n",
    "           #Geo(join(ds_path, \"snap_standford/Gowalla_epsilon.csv\"), \"Gowalla\", lamb=1.001, k=100)]\n",
    "\n",
    "for ds in dataset:\n",
    "    experiment(ds, resilient_k_param, plot_path, result_path, epsilon=eps_dict[(ds.lamb, len(pair1))])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Birch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset = [Birch(join(ds_path, \"birch/shrink_birch1_epsilon.csv\"), subset=1, lamb=1.1, k=10),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch1_epsilon.csv\"), subset=1, lamb=1.1, k=20),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch2_epsilon.csv\"), subset=2, lamb=1.1, k=10),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch2_epsilon.csv\"), subset=2, lamb=1.1, k=20),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch3_epsilon.csv\"), subset=3, lamb=1.1, k=10),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch3_epsilon.csv\"), subset=3, lamb=1.1, k=20)]\n",
    "\n",
    "for ds in dataset:\n",
    "    experiment(ds, resilient_k_param, plot_path, result_path, epsilon=eps_dict[(ds.lamb, len(pair1))])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.High dim "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset = [HighDim(join(ds_path, \"high_dim/dim032_epsilon.csv\"), dim=32, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim032_epsilon.csv\"), dim=32, lamb=1.1, k=20),\n",
    "           HighDim(join(ds_path, \"high_dim/dim064_epsilon.csv\"), dim=64, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim064_epsilon.csv\"), dim=64, lamb=1.1, k=20),\n",
    "           HighDim(join(ds_path, \"high_dim/dim128_epsilon.csv\"), dim=128, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim128_epsilon.csv\"), dim=128, lamb=1.1, k=20)]\n",
    "for ds in dataset:\n",
    "    experiment(ds, resilient_k_param, plot_path, result_path, epsilon=eps_dict[(ds.lamb, len(pair1))])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from os.path import join, isdir, isfile\n",
    "from os import mkdir\n",
    "from src.datasets import *\n",
    "import pickle"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ds_path = \"./dataset\"\n",
    "resilient_k_param ={\"alpha\": [0.5, 1.0], \n",
    "                    \"beta\": [0.5, 1.0],\n",
    "                    \"algorithm\": [\"gonz\", \"carv\"]}\n",
    "\n",
    "to_eval = [Uber(join(ds_path, \"uber/uber_epsilon.csv\"), lamb=1.1, k=10),\n",
    "           Uber(join(ds_path, \"uber/uber_epsilon.csv\"), lamb=1.1, k=20),\n",
    "           Geo(join(ds_path, \"snap_standford/Brightkite_epsilon.csv\"), \"Brightkite\", lamb=1.1, k=50), \n",
    "           Geo(join(ds_path, \"snap_standford/Brightkite_epsilon.csv\"), \"Brightkite\", lamb=1.1, k=100),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch1_epsilon.csv\"), subset=1, lamb=1.1, k=10),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch1_epsilon.csv\"), subset=1, lamb=1.1, k=20),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch2_epsilon.csv\"), subset=2, lamb=1.1, k=10),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch2_epsilon.csv\"), subset=2, lamb=1.1, k=20),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch3_epsilon.csv\"), subset=3, lamb=1.1, k=10),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch3_epsilon.csv\"), subset=3, lamb=1.1, k=20), \n",
    "           HighDim(join(ds_path, \"high_dim/dim032_epsilon.csv\"), dim=32, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim032_epsilon.csv\"), dim=32, lamb=1.1, k=20),\n",
    "           HighDim(join(ds_path, \"high_dim/dim064_epsilon.csv\"), dim=64, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim064_epsilon.csv\"), dim=64, lamb=1.1, k=20),\n",
    "           HighDim(join(ds_path, \"high_dim/dim128_epsilon.csv\"), dim=128, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim128_epsilon.csv\"), dim=128, lamb=1.1, k=20) \n",
    "           ]\n",
    "result_path = \"./results/baseline\"\n",
    "random_seed = [5331,5332,5333]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Gonz algorithm"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.evaluation import Gonz_Approx_Algo\n",
    "result_path = \"./results/baseline\"\n",
    "for ds in to_eval:\n",
    "    if basline_set_random_seed:\n",
    "        loop_content = random_seed\n",
    "    else:\n",
    "        loop_content = [None] * len(random_seed)\n",
    "    for i, seed in enumerate(loop_content):\n",
    "        print(f\"Processing {ds.name} dataset with k={ds.k}, algorithm=gonz_only\")\n",
    "        pair1, pair2 = ds.load()\n",
    "        model1 = Gonz_Approx_Algo(pair1, ds.k, seed)\n",
    "        center1, cluster1 = model1.clustering()\n",
    "        model2 = Gonz_Approx_Algo(pair2, ds.k, seed)\n",
    "        center2, cluster2 = model2.clustering()\n",
    "\n",
    "        if seed == None:\n",
    "            seed = \"None\" + \"_\" + str(i)\n",
    "        seed_result_path = join(result_path, f'{seed}')\n",
    "        if not isdir(seed_result_path):\n",
    "            mkdir(seed_result_path)\n",
    "        this_result_path = join(seed_result_path, f'{ds.name}')\n",
    "        if not isdir(this_result_path):\n",
    "            mkdir(this_result_path)\n",
    "        with open(join(this_result_path, f\"{ds.name}_resilient_{ds.k}_gonz_only.pickle\"), 'wb') as output_file:\n",
    "            pickle.dump((center1, cluster1, center2, cluster2), output_file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Carve algorithm only"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.evaluation import CarvingAlgorithm\n",
    "result_path = \"./results/baseline\"\n",
    "for ds in to_eval:\n",
    "    if basline_set_random_seed:\n",
    "        loop_content = random_seed\n",
    "    else:\n",
    "        loop_content = [None] * len(random_seed)\n",
    "    for i, seed in enumerate(loop_content):\n",
    "        print(f\"Processing {ds.name} dataset with k={ds.k}, algorithm=gonz_only\")\n",
    "        pair1, pair2 = ds.load()\n",
    "        \n",
    "        model1 = CarvingAlgorithm(pair1, seed=seed)\n",
    "        best_r = model1.find_minimum_R(ds.k)\n",
    "        center1, cluster1 = model1.carve(best_r, ds.k)\n",
    "        model2 = CarvingAlgorithm(pair2, seed=seed)\n",
    "        best_r = model2.find_minimum_R(ds.k)\n",
    "        center2, cluster2 = model2.carve(best_r, ds.k)\n",
    "        if seed == None:\n",
    "            seed = \"None\" + \"_\" + str(i)\n",
    "        seed_result_path = join(result_path, f'{seed}')\n",
    "        if not isdir(seed_result_path):\n",
    "            mkdir(seed_result_path)\n",
    "        this_result_path = join(seed_result_path, f'{ds.name}')\n",
    "        if not isdir(this_result_path):\n",
    "            mkdir(this_result_path)\n",
    "        with open(join(this_result_path, f\"{ds.name}_resilient_{ds.k}_carve_only.pickle\"), 'wb') as output_file:\n",
    "            pickle.dump((center1, cluster1, center2, cluster2), output_file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Use directory structure from one drive for results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# alter these 2 variable to generate different plot after generation\n",
    "basline_set_random_seed = True\n",
    "resilient_set_random_seed = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.plot_helper import plot_bar\n",
    "\n",
    "results ={\n",
    "    # dataset : k, seed, a, b (need to access resilient and baseline)\n",
    "    'Birch1': [[10, 20],[5331,5332,5333], (0.5, 1.0), (0.5, 1.0)],\n",
    "    'Birch2': [[10, 20],[5331,5332,5333], (0.5, 1.0), (0.5, 1.0)],\n",
    "    'Birch3': [[10, 20],[5331,5332,5333], (0.5, 1.0), (0.5, 1.0)],\n",
    "    'Brightkite': [[50,100],[5331,5332,5333], (0.5, 1.0), (0.5, 1.0)],\n",
    "    'HighDim32' : [[10,20],[5331,5332,5333], (0.5, 1.0), (0.5, 1.0)],\n",
    "    'HighDim64' : [[10,20],[5331,5332,5333], (0.5, 1.0), (0.5, 1.0)],\n",
    "    'HighDim128' : [[10,20],[5331,5332,5333], (0.5, 1.0), (0.5, 1.0)],\n",
    "    'Uber': [[10, 20], [5331,5332,5333], (0.5, 1.0), (0.5, 1.0)]\n",
    "}\n",
    "resilient_k_models = [\"gonz\", \"carv\"]\n",
    "baseline_models = [\"gonz\", \"carve\"]\n",
    "cluster_results = {}\n",
    "\n",
    "results_path = \"./results\"\n",
    "eval_path = f\"./{results_path}/log/\"\n",
    "if not isdir(eval_path):\n",
    "    mkdir(eval_path)\n",
    "\n",
    "plot_bar(results, resilient_k_models, baseline_models, results_path, eval_path, basline_set_random_seed, resilient_set_random_seed)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
