{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVPzFZH4gE_B"
   },
   "source": [
    "# COMP5331 Group 6 Project: Resilient k-Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T18:16:52.950989Z",
     "start_time": "2024-11-09T18:16:52.875469Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocess.process_uber import process_uber\n",
    "ds_path = \"dataset/uber/uber-raw-data-jun14.csv\"\n",
    "save_path = \"dataset/uber/\"\n",
    "process_uber(ds_path, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocess.process_geo import process_geo\n",
    "ds_names = [\"Brightkite\", \"Gowalla\"]\n",
    "for ds_name in ds_names:\n",
    "    print(f\"Processing {ds_name} dataset\")\n",
    "    ds_path = f\"dataset/snap_standford/{ds_name}_totalCheckins.txt\"\n",
    "    save_path = \"dataset/snap_standford/\"\n",
    "    process_geo(ds_path, save_path, ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocess.process_birch import process_birch\n",
    "ds_name = \"birch1\"\n",
    "ds_path = f\"dataset/birch/{ds_name}.txt\"\n",
    "save_path = \"dataset/birch\"\n",
    "process_birch(ds_path, save_path, ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocess.process_high_dim import process_high_dim\n",
    "dims = ['032', '064', '128']\n",
    "\n",
    "for dim in dims:\n",
    "    ds_name = \"dim\"+dim\n",
    "    ds_path = f\"/dataset/high_dim/{ds_name}.txt\"\n",
    "    save_path = \"/dataset/high_dim\"\n",
    "    process_high_dim(ds_path, save_path, ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T18:53:02.503761Z",
     "start_time": "2024-11-09T18:52:58.137451Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.plot_helper import plot_data\n",
    "from src.datasets import *\n",
    "from os.path import join, isdir\n",
    "from os import mkdir\n",
    "ds_path = \"./dataset\"\n",
    "dataset = [Uber(join(ds_path, \"uber/uber_epsilon.csv\"), lamb=1.1, k=10), \n",
    "           Geo(join(ds_path, \"snap_standford/Brightkite_epsilon.csv\"), \"Brightkite\", lamb=1.1, k=50),\n",
    "           Geo(join(ds_path, \"snap_standford/Gowalla_epsilon.csv\"), \"Gowalla\", lamb=1.001, k=50),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch1_epsilon.csv\"), subset=1, lamb=1.1, k=10),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch2_epsilon.csv\"), subset=2, lamb=1.1, k=10),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch3_epsilon.csv\"), subset=3, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim032_epsilon.csv\"), dim=32, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim064_epsilon.csv\"), dim=64, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim128_epsilon.csv\"), dim=128, lamb=1.1, k=10)\n",
    "           ]\n",
    "plot_path = \"./dataset/plot\"\n",
    "if not isdir(plot_path):\n",
    "    mkdir(plot_path)\n",
    "for ds in dataset:\n",
    "    pair1, pair2 = ds.load()\n",
    "    plot_data(pair1, pair2, plot_path, ds.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate possible range for epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "def pick_epsilon(lambda_, n):\n",
    "    epsilion_lower_bound = (3 * np.log(ds.lamb) * np.log(n) ** 2) / n\n",
    "    epsilion_upper_bound = 1 \n",
    "    epsilon = np.random.uniform(epsilion_lower_bound, epsilion_upper_bound)\n",
    "    while (1+epsilon) >= lambda_:\n",
    "        epsilon = np.random.uniform(epsilion_lower_bound, epsilion_upper_bound)\n",
    "    \n",
    "    return epsilon\n",
    "dataset = [Uber(join(ds_path, \"uber/uber_epsilon.csv\"), lamb=1.1, k=10), \n",
    "           Geo(join(ds_path, \"snap_standford/Brightkite_epsilon.csv\"), \"Brightkite\", lamb=1.1, k=50),\n",
    "           Geo(join(ds_path, \"snap_standford/Gowalla_epsilon.csv\"), \"Gowalla\", lamb=1.001, k=50),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch1_epsilon.csv\"), subset=1, lamb=1.1, k=10),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch2_epsilon.csv\"), subset=2, lamb=1.1, k=10),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch3_epsilon.csv\"), subset=3, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim032_epsilon.csv\"), dim=32, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim064_epsilon.csv\"), dim=64, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim128_epsilon.csv\"), dim=128, lamb=1.1, k=10)\n",
    "           ]\n",
    "# Create epsilon for each lambda and data size combination\n",
    "\n",
    "eps_dict = {}\n",
    "for ds in dataset:\n",
    "    lamb = ds.lamb\n",
    "    pair1, pair2 = ds.load()\n",
    "    n = len(pair1)\n",
    "    print(f'{lamb}, {n}, {pick_epsilon(lamb, n)}')\n",
    "    if lamb == 1.1:\n",
    "        epsilon = 0.05\n",
    "    elif lamb == 1.001:\n",
    "        epsilon = 0.0005\n",
    "    eps_dict[(lamb,n)] = epsilon\n",
    "pprint(eps_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alter these 2 variable to generate different result \n",
    "basline_set_random_seed = False\n",
    "resilient_set_random_seed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-resillient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T18:17:41.107402Z",
     "start_time": "2024-11-09T18:17:40.965648Z"
    },
    "id": "yw95uRdmfuqc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.resilient_k import resilient_k_center\n",
    "from src.datasets import *\n",
    "from os.path import join, isdir\n",
    "from os import mkdir, makedirs\n",
    "from src.plot_helper import plot_cluster_result\n",
    "import pickle\n",
    "#import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T18:17:42.699284Z",
     "start_time": "2024-11-09T18:17:42.573843Z"
    }
   },
   "outputs": [],
   "source": [
    "def experiment(ds, resilient_k_param, plot_path, result_path, epsilon):\n",
    "    for algo in resilient_k_param[\"algorithm\"]:\n",
    "        for alpha in resilient_k_param[\"alpha\"]:\n",
    "            for beta in resilient_k_param[\"beta\"]:  \n",
    "                if resilient_set_random_seed:\n",
    "                    loop_content = resilient_k_param[\"seed\"]\n",
    "                else:\n",
    "                    loop_content = [None] * len(resilient_k_param[\"seed\"])\n",
    "                for i, seed in enumerate(loop_content):   \n",
    "                    \n",
    "                    print(f\"Processing {ds.name} dataset with k={ds.k}, lamb={ds.lamb}, alpha={alpha}, beta={beta}, algorithm={algo}, seed={seed}, epsilon={epsilon}\")\n",
    "                    pair1, pair2 = ds.load()                    \n",
    "                    \n",
    "                    model1 = resilient_k_center(pair1, k=ds.k, lamb=ds.lamb, epsilon=epsilon, alpha=alpha, beta=beta, algorithm=algo, seed=seed)\n",
    "                    #start_time = time.process_time_ns()\n",
    "                    center1, cluster1 = model1.resilient_k_center()\n",
    "                    #end_time = time.process_time_ns()\n",
    "                    #time_taken1 = end_time - start_time\n",
    "                    \n",
    "                    model2 = resilient_k_center(pair2, k=ds.k, lamb=ds.lamb, epsilon=epsilon, alpha=alpha, beta=beta, algorithm=algo, seed=seed)\n",
    "                    #start_time = time.process_time_ns()\n",
    "                    center2, cluster2 = model2.resilient_k_center()\n",
    "        \n",
    "                    #end_time = time.process_time_ns()\n",
    "                    #time_taken2 = end_time - start_time\n",
    "                    if seed == None:\n",
    "                        seed = \"None\" + \"_\" + str(i)\n",
    "                    seed_result_path = join(result_path, f'{seed}')\n",
    "                    if not isdir(seed_result_path):\n",
    "                        mkdir(seed_result_path)\n",
    "                    this_result_path = join(seed_result_path, f'{ds.name}')\n",
    "                    if not isdir(this_result_path):\n",
    "                        mkdir(this_result_path)\n",
    "                        \n",
    "                    with open(join(this_result_path, f\"{ds.name}_resilient_{ds.k}_{algo}({alpha}_{beta}).pickle\"), 'wb') as output_file:\n",
    "                        #pickle.dump((center1, cluster1, center2, cluster2, time_taken1, time_taken2), output_file)\n",
    "                        pickle.dump((center1, cluster1, center2, cluster2), output_file)\n",
    "                        \n",
    "                    # pt1 = np.asarray([c[0] for c in cluster1])\n",
    "                    # pt2 = np.asarray([c[0] for c in cluster2])\n",
    "                    # label1 = [c[1] for c in cluster1]\n",
    "                    # label2 = [c[1] for c in cluster2]\n",
    "                    \n",
    "                    # seed_plot_path = join(plot_path, f'{seed}')\n",
    "                    # if not isdir(seed_plot_path):\n",
    "                    #     mkdir(seed_plot_path)\n",
    "                    # this_plot_path = join(seed_plot_path, ds.name)\n",
    "                    # if not isdir(this_plot_path):\n",
    "                    #     mkdir(this_plot_path)\n",
    "                        \n",
    "                    # plot_cluster_result(pt1, pt2, label1, label2, this_plot_path, ds, algo, alpha, beta)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T18:57:53.025724Z",
     "start_time": "2024-11-09T18:57:52.897822Z"
    }
   },
   "outputs": [],
   "source": [
    "ds_path = \"./dataset\"\n",
    "resilient_k_param ={\"alpha\": [0.5, 1.0], \n",
    "                    \"beta\": [0.5, 1.0],\n",
    "                    \"algorithm\": [\"gonz\", \"carv\"],\n",
    "                    \"seed\": [5331,5332,5333]}\n",
    "plot_path = \"./results/plot/resilient_k/\"\n",
    "result_path = \"./results/resilient_k/\"\n",
    "if not isdir(plot_path):\n",
    "    makedirs(plot_path)\n",
    "if not isdir(result_path):\n",
    "    makedirs(result_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T16:50:52.992630Z",
     "start_time": "2024-11-09T16:48:33.587830Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1p2F0xu0re6q",
    "outputId": "1e9ad1cf-9e94-49a2-8f06-b3189d88d28b"
   },
   "outputs": [],
   "source": [
    "dataset = [Uber(join(ds_path, \"uber/uber_epsilon.csv\"), lamb=1.1, k=10),\n",
    "           Uber(join(ds_path, \"uber/uber_epsilon.csv\"), lamb=1.1, k=20)]\n",
    "\n",
    "for ds in dataset:\n",
    "    experiment(ds, resilient_k_param, plot_path, result_path, epsilon=eps_dict[(ds.lamb, len(pair1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Brightkite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T17:32:26.313519Z",
     "start_time": "2024-11-09T16:50:53.008716Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = [Geo(join(ds_path, \"snap_standford/Brightkite_epsilon.csv\"), \"Brightkite\", lamb=1.1, k=50), \n",
    "           Geo(join(ds_path, \"snap_standford/Brightkite_epsilon.csv\"), \"Brightkite\", lamb=1.1, k=100)]#,\n",
    "           #Geo(join(ds_path, \"snap_standford/Gowalla_epsilon.csv\"), \"Gowalla\", lamb=1.001, k=50),\n",
    "           #Geo(join(ds_path, \"snap_standford/Gowalla_epsilon.csv\"), \"Gowalla\", lamb=1.001, k=100)]\n",
    "\n",
    "for ds in dataset:\n",
    "    experiment(ds, resilient_k_param, plot_path, result_path, epsilon=eps_dict[(ds.lamb, len(pair1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Birch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:03:45.775101Z",
     "start_time": "2024-11-09T18:57:59.839801Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = [Birch(join(ds_path, \"birch/shrink_birch1_epsilon.csv\"), subset=1, lamb=1.1, k=10),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch1_epsilon.csv\"), subset=1, lamb=1.1, k=20),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch2_epsilon.csv\"), subset=2, lamb=1.1, k=10),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch2_epsilon.csv\"), subset=2, lamb=1.1, k=20),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch3_epsilon.csv\"), subset=3, lamb=1.1, k=10),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch3_epsilon.csv\"), subset=3, lamb=1.1, k=20)]\n",
    "\n",
    "for ds in dataset:\n",
    "    experiment(ds, resilient_k_param, plot_path, result_path, epsilon=eps_dict[(ds.lamb, len(pair1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.High dim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T18:14:01.345791Z",
     "start_time": "2024-11-09T17:32:26.362523Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = [HighDim(join(ds_path, \"high_dim/dim032_epsilon.csv\"), dim=32, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim032_epsilon.csv\"), dim=32, lamb=1.1, k=20),\n",
    "           HighDim(join(ds_path, \"high_dim/dim064_epsilon.csv\"), dim=64, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim064_epsilon.csv\"), dim=64, lamb=1.1, k=20),\n",
    "           HighDim(join(ds_path, \"high_dim/dim128_epsilon.csv\"), dim=128, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim128_epsilon.csv\"), dim=128, lamb=1.1, k=20)]\n",
    "for ds in dataset:\n",
    "    experiment(ds, resilient_k_param, plot_path, result_path, epsilon=eps_dict[(ds.lamb, len(pair1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T19:39:19.097571Z",
     "start_time": "2024-11-09T19:39:18.949951Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.evaluation import Metrics, Clustering\n",
    "from src.testing.find_pair_assignment import find_pair_assign\n",
    "from os.path import join, isdir, isfile\n",
    "from os import mkdir\n",
    "from src.datasets import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = \"./dataset\"\n",
    "resilient_k_param ={\"alpha\": [0.5, 1.0], \n",
    "                    \"beta\": [0.5, 1.0],\n",
    "                    \"algorithm\": [\"gonz\", \"carv\"]}\n",
    "\n",
    "to_eval = [Uber(join(ds_path, \"uber/uber_epsilon.csv\"), lamb=1.1, k=10),\n",
    "           Uber(join(ds_path, \"uber/uber_epsilon.csv\"), lamb=1.1, k=20),\n",
    "           Geo(join(ds_path, \"snap_standford/Brightkite_epsilon.csv\"), \"Brightkite\", lamb=1.1, k=50), \n",
    "           Geo(join(ds_path, \"snap_standford/Brightkite_epsilon.csv\"), \"Brightkite\", lamb=1.1, k=100),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch1_epsilon.csv\"), subset=1, lamb=1.1, k=10),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch1_epsilon.csv\"), subset=1, lamb=1.1, k=20),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch2_epsilon.csv\"), subset=2, lamb=1.1, k=10),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch2_epsilon.csv\"), subset=2, lamb=1.1, k=20),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch3_epsilon.csv\"), subset=3, lamb=1.1, k=10),\n",
    "           Birch(join(ds_path, \"birch/shrink_birch3_epsilon.csv\"), subset=3, lamb=1.1, k=20), \n",
    "           HighDim(join(ds_path, \"high_dim/dim032_epsilon.csv\"), dim=32, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim032_epsilon.csv\"), dim=32, lamb=1.1, k=20),\n",
    "           HighDim(join(ds_path, \"high_dim/dim064_epsilon.csv\"), dim=64, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim064_epsilon.csv\"), dim=64, lamb=1.1, k=20),\n",
    "           HighDim(join(ds_path, \"high_dim/dim128_epsilon.csv\"), dim=128, lamb=1.1, k=10),\n",
    "           HighDim(join(ds_path, \"high_dim/dim128_epsilon.csv\"), dim=128, lamb=1.1, k=20) \n",
    "           ]\n",
    "result_path = \"./results/baseline\"\n",
    "random_seed = [5331,5332,5333]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Gonz algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import Gonz_Approx_Algo\n",
    "result_path = \"./results/baseline\"\n",
    "for ds in to_eval:\n",
    "    if basline_set_random_seed:\n",
    "        loop_content = random_seed\n",
    "    else:\n",
    "        loop_content = [None] * len(random_seed)\n",
    "    for i, seed in enumerate(loop_content):\n",
    "        print(f\"Processing {ds.name} dataset with k={ds.k}, algorithm=gonz_only\")\n",
    "        pair1, pair2 = ds.load()\n",
    "        model1 = Gonz_Approx_Algo(pair1, ds.k, seed)\n",
    "        center1, cluster1 = model1.clustering()\n",
    "        model2 = Gonz_Approx_Algo(pair2, ds.k, seed)\n",
    "        center2, cluster2 = model2.clustering()\n",
    "\n",
    "        if seed == None:\n",
    "            seed = \"None\" + \"_\" + str(i)\n",
    "        seed_result_path = join(result_path, f'{seed}')\n",
    "        if not isdir(seed_result_path):\n",
    "            mkdir(seed_result_path)\n",
    "        this_result_path = join(seed_result_path, f'{ds.name}')\n",
    "        if not isdir(this_result_path):\n",
    "            mkdir(this_result_path)\n",
    "        with open(join(this_result_path, f\"{ds.name}_resilient_{ds.k}_gonz_only.pickle\"), 'wb') as output_file:\n",
    "            pickle.dump((center1, cluster1, center2, cluster2), output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Carve algorithm only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import CarvingAlgorithm\n",
    "result_path = \"./results/baseline\"\n",
    "for ds in to_eval:\n",
    "    if basline_set_random_seed:\n",
    "        loop_content = random_seed\n",
    "    else:\n",
    "        loop_content = [None] * len(random_seed)\n",
    "    for i, seed in enumerate(loop_content):\n",
    "        print(f\"Processing {ds.name} dataset with k={ds.k}, algorithm=gonz_only\")\n",
    "        pair1, pair2 = ds.load()\n",
    "        \n",
    "        model1 = CarvingAlgorithm(pair1, seed=seed)\n",
    "        best_r = model1.find_minimum_R(ds.k)\n",
    "        center1, cluster1 = model1.carve(best_r, ds.k)\n",
    "        model2 = CarvingAlgorithm(pair2, seed=seed)\n",
    "        best_r = model2.find_minimum_R(ds.k)\n",
    "        center2, cluster2 = model2.carve(best_r, ds.k)\n",
    "        if seed == None:\n",
    "            seed = \"None\" + \"_\" + str(i)\n",
    "        seed_result_path = join(result_path, f'{seed}')\n",
    "        if not isdir(seed_result_path):\n",
    "            mkdir(seed_result_path)\n",
    "        this_result_path = join(seed_result_path, f'{ds.name}')\n",
    "        if not isdir(this_result_path):\n",
    "            mkdir(this_result_path)\n",
    "        with open(join(this_result_path, f\"{ds.name}_resilient_{ds.k}_carve_only.pickle\"), 'wb') as output_file:\n",
    "            pickle.dump((center1, cluster1, center2, cluster2), output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. HS algorithm only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import HSAlgorithm\n",
    "result_path = \"./results/baseline\"\n",
    "for ds in to_eval:\n",
    "    if basline_set_random_seed:\n",
    "        loop_content = random_seed\n",
    "    else:\n",
    "        loop_content = [None] * len(random_seed)\n",
    "    for i, seed in enumerate(loop_content):\n",
    "        print(f\"Processing {ds.name} dataset with k={ds.k}, algorithm=gonz_only\")\n",
    "        pair1, pair2 = ds.load()\n",
    "        \n",
    "        model1 = HSAlgorithm(pair1,ds.k)\n",
    "        center1, cluster1 = model1.hs_algorithm(pair1, ds.k, seed=seed)\n",
    "        model2 = HSAlgorithm(pair1,ds.k)\n",
    "        center2, cluster2 = model2.hs_algorithm(pair2, ds.k, seed=seed)\n",
    "        \n",
    "        if seed == None:\n",
    "            seed = \"None\" + \"_\" + str(i)\n",
    "        seed_result_path = join(result_path, f'{seed}')\n",
    "        if not isdir(seed_result_path):\n",
    "            mkdir(seed_result_path)\n",
    "        this_result_path = join(seed_result_path, f'{ds.name}')\n",
    "        if not isdir(this_result_path):\n",
    "            mkdir(this_result_path)\n",
    "        with open(join(this_result_path, f\"{ds.name}_resilient_{ds.k}_hs_only.pickle\"), 'wb') as output_file:\n",
    "            pickle.dump((center1, cluster1, center2, cluster2), output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Use directory structure from one drive for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T15:07:13.184352Z",
     "start_time": "2024-11-12T15:07:13.177840Z"
    }
   },
   "outputs": [],
   "source": [
    "# alter these 2 variable to generate different plot after generation\n",
    "basline_set_random_seed = True\n",
    "resilient_set_random_seed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T15:43:15.106270Z",
     "start_time": "2024-11-12T15:42:23.085937Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.evaluation import Metrics, Clustering\n",
    "from os.path import join, isdir, isfile\n",
    "from os import mkdir\n",
    "from src.datasets import *\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "results ={\n",
    "    # dataset : k, seed, a, b (need to access resilient and baseline)\n",
    "    'Birch1': [[10, 20],[5331,5332,5333], (0.5, 1.0), (0.5, 1.0)],\n",
    "    'Birch2': [[10, 20],[5331,5332,5333], (0.5, 1.0), (0.5, 1.0)],\n",
    "    'Birch3': [[10, 20],[5331,5332,5333], (0.5, 1.0), (0.5, 1.0)],\n",
    "    'Brightkite': [[50,100],[5331,5332,5333], (0.5, 1.0), (0.5, 1.0)],\n",
    "    'HighDim32' : [[10,20],[5331,5332,5333], (0.5, 1.0), (0.5, 1.0)],\n",
    "    'HighDim64' : [[10,20],[5331,5332,5333], (0.5, 1.0), (0.5, 1.0)],\n",
    "    'HighDim128' : [[10,20],[5331,5332,5333], (0.5, 1.0), (0.5, 1.0)],\n",
    "    'Uber': [[10, 20], [5331,5332,5333], (0.5, 1.0), (0.5, 1.0)]\n",
    "}\n",
    "resilient_k_models = [\"gonz\", \"carv\"]\n",
    "baseline_models = [\"gonz\", \"carve\", \"hs\"]\n",
    "cluster_results = {}\n",
    "metric = Metrics()\n",
    "\n",
    "results_path = \"./results\"\n",
    "eval_path = f\"./{results_path}/log/\"\n",
    "if not isdir(eval_path):\n",
    "    mkdir(eval_path)\n",
    "\n",
    "for ds_name, params in results.items():\n",
    "    \n",
    "    cluster_result = {}\n",
    "    # 1 plot for each k\n",
    "    flag = False\n",
    "    if flag and isfile(join(eval_path, f\"{ds_name}_baseline_exp_result.csv\")):\n",
    "        ds_result = pd.read_csv(join(eval_path, f\"{ds_name}_baseline_exp_result.csv\"))\n",
    "    else:\n",
    "        ds_result = pd.DataFrame() #  columns=[\"k\", \"algo\", \"fraction_changed\", \"sol_cost_1\", \"sol_cost_2\", \"num_cluster_1\", \"num_cluster_2\"]\n",
    "        \n",
    "    for k in params[0]:\n",
    "        cluster_result[k] = {}\n",
    "            # get baseline model here\n",
    "        for model in baseline_models:\n",
    "            model_result = {}\n",
    "            fraction_result_list = []\n",
    "            sol_cost_result_list = []\n",
    "            num_cluster_result_list = []\n",
    "            if basline_set_random_seed:\n",
    "                loop_content = params[1]\n",
    "            else:\n",
    "                loop_content = [None] * len(params[1])\n",
    "            for i, seed in enumerate(loop_content):\n",
    "\n",
    "                if seed == None:\n",
    "                    seed = \"None\" + \"_\" + str(i)\n",
    "                result_path = f\"./{results_path}/baseline/{seed}/{ds_name}\"\n",
    "                with open(join(result_path, f\"{ds_name}_resilient_{k}_{model}_only.pickle\"), 'rb') as input_file:\n",
    "                    center1, cluster1, center2, cluster2 = pickle.load(input_file)\n",
    "                    cluster1 = [(c[0], np.array([[c[1]]])) for c in cluster1]\n",
    "                    cluster2 = [(c[0], np.array([[c[1]]])) for c in cluster2]\n",
    "                    fraction_changed, sol_cost, num_cluster = metric.evaluate(old_points=cluster1, old_medoids=center1, new_points=cluster2, new_medoids=center2, epsilon=0.3)\n",
    "\n",
    "                    fraction_result_list.append(fraction_changed)\n",
    "                    sol_cost_result_list.append(sol_cost[1])\n",
    "                    num_cluster_result_list.append(num_cluster[1])\n",
    "       \n",
    "            fraction_result_list = np.array(fraction_result_list)\n",
    "            sol_cost_result_list = np.array(sol_cost_result_list)\n",
    "            num_cluster_result_list = np.array(num_cluster_result_list)\n",
    "            \n",
    "            fraction_mean, fraction_std = np.mean(fraction_result_list), np.std(fraction_result_list)\n",
    "            sol_cost_mean, sol_cost_std = np.mean(sol_cost_result_list, axis=0), np.std(sol_cost_result_list, axis=0)\n",
    "            num_cluster_mean, num_cluster_std = np.mean(num_cluster_result_list, axis=0), np.std(num_cluster_result_list, axis=0)\n",
    "            \n",
    "            ds_result = pd.concat([ds_result, pd.DataFrame.from_records([{\n",
    "                    \"k\": k, \n",
    "                    \"algo\": f\"Baseline {model.title().replace('Hs', 'HS')}\", \n",
    "                    \"fraction_changed_mean\": fraction_mean, \n",
    "                    \"fraction_changed_std\": fraction_std,\n",
    "                    \"sol_cost_1\": sol_cost_result_list[0],\n",
    "                    \"sol_cost_2\": sol_cost_result_list[1],\n",
    "                    \"sol_cost_mean\": sol_cost_mean,\n",
    "                    \"sol_cost_std\": sol_cost_std,\n",
    "                    \"num_cluster_1\": num_cluster_result_list[0],\n",
    "                    \"num_cluster_2\": num_cluster_result_list[1],\n",
    "                    \"num_cluster_mean\": num_cluster_mean,\n",
    "                    \"num_cluster_std\": num_cluster_std\n",
    "                }])], ignore_index=True)\n",
    "            \n",
    "                \n",
    "            cluster_result[k][f\"Baseline {model.title().replace('Hs', 'HS')}\"] = {\n",
    "                        \"fraction_of_points_changing_cluster\": [fraction_mean, fraction_std],\n",
    "                        \"solution_cost\": [sol_cost_mean, sol_cost_std],\n",
    "                        \"number_of_clusters\": [num_cluster_mean, num_cluster_std]\n",
    "                    }\n",
    "            \n",
    "            # construct resilient model result below\n",
    "        # for 1 model result below\n",
    "        for model in resilient_k_models:\n",
    "            for a in params[2]:\n",
    "                for b in params[3]:\n",
    "                    fraction_result_list = []\n",
    "                    sol_cost_result_list = []\n",
    "                    num_cluster_result_list = []\n",
    "                    if resilient_set_random_seed:\n",
    "                        loop_content = params[1]\n",
    "                    else:\n",
    "                        loop_content = [None] * len(params[1])\n",
    "                    for i, seed in enumerate(loop_content):\n",
    "                        if seed == None:\n",
    "                            seed = \"None\" + \"_\" + str(i)\n",
    "                        result_path = f\"./{results_path}/resilient_k/{seed}/{ds_name}/\"\n",
    "                        with open(join(result_path, f\"{ds_name}_resilient_{k}_{model}({a}_{b}).pickle\"), 'rb') as input_file:\n",
    "                            center1, cluster1, center2, cluster2 = pickle.load(input_file)\n",
    "                        fraction_changed, sol_cost, num_cluster = metric.evaluate(old_points=cluster1, old_medoids=center1, new_points=cluster2, new_medoids=center2, epsilon=0.3)\n",
    "\n",
    "                        fraction_result_list.append(fraction_changed)\n",
    "                        \n",
    "                        sol_cost_result_list.append(sol_cost[1])\n",
    "                        num_cluster_result_list.append(num_cluster[1])\n",
    "                        \n",
    "                    fraction_result_list = np.array(fraction_result_list)\n",
    "                    sol_cost_result_list = np.array(sol_cost_result_list)\n",
    "                    num_cluster_result_list = np.array(num_cluster_result_list)\n",
    "                    \n",
    "                    fraction_mean, fraction_std = np.mean(fraction_result_list), np.std(fraction_result_list)\n",
    "                    sol_cost_mean, sol_cost_std = np.mean(sol_cost_result_list, axis=0), np.std(sol_cost_result_list, axis=0)\n",
    "                    num_cluster_mean, num_cluster_std = np.mean(num_cluster_result_list, axis=0), np.std(num_cluster_result_list, axis=0)\n",
    "                        \n",
    "                    cluster_result[k][f'Con{model[0].title()}({a}, {b})'] = {\n",
    "                                \"fraction_of_points_changing_cluster\": [fraction_mean, fraction_std],\n",
    "                                \"solution_cost\": [sol_cost_mean, sol_cost_std],\n",
    "                                \"number_of_clusters\": [num_cluster_mean, num_cluster_std]\n",
    "                            }\n",
    "                    ds_result = pd.concat([ds_result, pd.DataFrame.from_records([{\n",
    "                    \"k\": k, \n",
    "                    \"algo\": f'Con{model[0].title()}({a}, {b})', \n",
    "                    \"fraction_changed_mean\": fraction_mean, \n",
    "                    \"fraction_changed_std\": fraction_std,\n",
    "                    \"sol_cost_1\": sol_cost_result_list[0],\n",
    "                    \"sol_cost_2\": sol_cost_result_list[1],\n",
    "                    \"sol_cost_mean\": sol_cost_mean,\n",
    "                    \"sol_cost_std\": sol_cost_std,\n",
    "                    \"num_cluster_1\": num_cluster_result_list[0],\n",
    "                    \"num_cluster_2\": num_cluster_result_list[1],\n",
    "                    \"num_cluster_mean\": num_cluster_mean,\n",
    "                    \"num_cluster_std\": num_cluster_std\n",
    "                    }])], ignore_index=True)\n",
    "    flag = True\n",
    "    for key in [\"fraction_of_points_changing_cluster\", \"solution_cost\", \"number_of_clusters\"]:\n",
    "        model_results_means= []\n",
    "        model_results_stds = []\n",
    "        model_name_list = []\n",
    "        # get model name\n",
    "        for k in cluster_result:\n",
    "            for model in cluster_result[k]:\n",
    "                model_name_list.append(model)\n",
    "            break\n",
    "        for k in cluster_result:\n",
    "            model_result_mean = []\n",
    "            model_result_std = []\n",
    "            for model in cluster_result[k]:\n",
    "                \n",
    "                model_result_mean.append(cluster_result[k][model][key][0])\n",
    "                model_result_std.append(cluster_result[k][model][key][1])\n",
    "            model_results_means.append(model_result_mean)\n",
    "            model_results_stds.append(model_result_std)\n",
    "        means = np.vstack(model_results_means).T\n",
    "        stds = np.vstack(model_results_stds).T\n",
    "        \n",
    "        x = np.arange(len(cluster_result))  # the label locations\n",
    "        width = 0.075  # the width of the bars\n",
    "        fig = plt.figure()\n",
    "        # Create bars for each set of data\n",
    "        for i, (fraction, error) in enumerate(zip(means, stds)):\n",
    "            plt.bar(x + i * width, fraction, width, label=model_name_list[i], yerr=error, capsize=5)\n",
    "\n",
    "        # Add some text for labels, title, and custom x-axis tick labels, etc.\n",
    "        plt.xlabel('k')\n",
    "        #plt.ylabel(f'{ds_name} {key}'.replace('_', ' ').title())\n",
    "        plt.title(f'{ds_name} {key}'.replace('_', ' ').title())\n",
    "        plt.xticks(x + width * (len(model_name_list) - 1) / 2, cluster_result.keys())\n",
    "        lgd = plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        if key == \"fraction_of_points_changing_cluster\":\n",
    "            ax = plt.gca()\n",
    "            ax.set_ylim([0.0, 1.0])\n",
    "            ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0, decimals=0))\n",
    "            \n",
    "\n",
    "        \n",
    "        # Show the plot\n",
    "        if not isdir(join(results_path, \"plots\")):\n",
    "            mkdir(join(results_path, \"plots\"))\n",
    "        plot_dir = f\"./{results_path}/plots/{ds_name}\"\n",
    "        if not isdir(plot_dir):\n",
    "            mkdir(plot_dir)\n",
    "\n",
    "        \n",
    "    \n",
    "        plt.savefig(join(plot_dir, f\"{ds_name}_{key}.png\"), bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "\n",
    "    ds_result.to_csv(join(eval_path, f\"{ds_name}_exp_result.csv\"), index=False)                  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
