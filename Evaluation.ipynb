{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of clustering objectives\n",
    "Global objective: \n",
    "\n",
    "K-means:\n",
    "Object: minimize the sum of squared distance from each item to its nearest averaged center.\n",
    "K-centers:\n",
    "Object: minimize the maximum distance from each item to its nearest cluster centers\n",
    "k-medians:\n",
    "Object: minimize the sum of distance from each item to its nearest median. \n",
    "k-medoids:\n",
    "Object: minimize the sum of squared distance from each item to its nearest medoids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import math\n",
    "import copy\n",
    "import random \n",
    "import networkx as nx\n",
    "from scipy.linalg import eigh\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraction_points_changing_cluster(old_clusters, new_clusters):\n",
    "    changes = np.sum(old_clusters != new_clusters)\n",
    "    total_points = len(old_clusters)\n",
    "    return changes / total_points\n",
    "\n",
    "def solution_cost(points, clusters, medoids):\n",
    "    max_distance = 0\n",
    "    for i, point in enumerate(points):\n",
    "        medoid = medoids[clusters[i]]\n",
    "        distance = np.linalg.norm(point - points[medoid])\n",
    "        max_distance = max(max_distance, distance)\n",
    "    return max_distance\n",
    "\n",
    "def number_of_clusters(clusters):\n",
    "    \"\"\"Count the number of unique clusters formed.\"\"\"\n",
    "    return len(np.unique(clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient k-means\n",
    "Below: baseline resilient k-means algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient k-centers\n",
    "Below: baseline resilient k-centers algorithm\n",
    "- Gonzalez algorithm\n",
    "- Carving Algorithm\n",
    "- Greedy Strategy Works for k-Center Clustering with Outliers and Coreset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://github.com/TSunny007/Clustering/blob/master/notebooks/Gonzalez.ipynb\n",
    "def max_dist(data, clusters):\n",
    "    distances = np.zeros(len(data)) # we will keep a cumulative distance measure for all points\n",
    "    for cluster_id, cluster in enumerate(clusters):\n",
    "        for point_id, point in enumerate(data):\n",
    "            if distance.euclidean(point,cluster) == 0.0:\n",
    "                distances[point_id] = -math.inf # this point is already a cluster \n",
    "            if not math.isinf(distances[point_id]):\n",
    "                distances[point_id] = distances[point_id] + distance.euclidean(point,cluster) \n",
    "    return data[np.argmax(distances)]\n",
    "\n",
    "def norm_dist(data, clusters):\n",
    "    distances = np.zeros(len(data)) # we will keep a cumulative distance measure for all points\n",
    "    for point_id, point in enumerate(data):\n",
    "        for cluster_id, cluster in enumerate(clusters):\n",
    "            if distance.euclidean(point,cluster) == 0.0:\n",
    "                distances[point_id] = -math.inf # this point is already a cluster (obselete)\n",
    "            if not math.isinf(distances[point_id]):\n",
    "                # if a point is not obselete, then we add the distance to its specific bin\n",
    "                distances[point_id] = distances[point_id] + math.pow(distance.euclidean(point,cluster),2) \n",
    "                # return the point which is furthest away from all the other clusters\n",
    "    for distance_id, current_distance in enumerate(distances):\n",
    "        if not math.isinf(current_distance): \n",
    "            distances[distance_id] = math.sqrt(current_distance/len(data))\n",
    "    return data[np.argmax(distances)]\n",
    "\n",
    "def gonzalez(data, cluster_num, method = 'max'):\n",
    "    clusters = []\n",
    "    clusters.append(data[0]) # assign the first point to the first cluster\n",
    "    while len(clusters) < cluster_num:\n",
    "        if method is 'max':\n",
    "            clusters.append(max_dist(data, clusters)) \n",
    "        if method is 'norm':\n",
    "            clusters.append(norm_dist(data, clusters)) \n",
    "        # we add the furthest point from ALL current clusters\n",
    "    return (clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(point1, point2):\n",
    "    return np.linalg.norm(np.array(point1) - np.array(point2))\n",
    "\n",
    "def carve(points, R, k):\n",
    "    centers = []\n",
    "    uncovered_indices = set(range(len(points)))  # Indices of uncovered points\n",
    "\n",
    "    while uncovered_indices and len(centers) < k:\n",
    "        # Randomly select an uncovered point\n",
    "        idx = random.choice(list(uncovered_indices))\n",
    "        center = points[idx]\n",
    "        centers.append(center)\n",
    "\n",
    "        # Mark all points within distance R from the new center as covered\n",
    "        to_remove = []\n",
    "        for i in uncovered_indices:\n",
    "            if distance(center, points[i]) <= R:\n",
    "                to_remove.append(i)\n",
    "\n",
    "        # Remove covered points from uncovered set\n",
    "        uncovered_indices.difference_update(to_remove)\n",
    "\n",
    "    return centers\n",
    "\n",
    "\n",
    "def find_minimum_R(points, k, R_start, R_end, step=0.1):\n",
    "    best_R = None\n",
    "\n",
    "    R = R_start\n",
    "    while R <= R_end:\n",
    "        centers = carve(points, R, k)\n",
    "        if len(centers) <= k:  # Check if we opened at most k centers\n",
    "            best_R = R  # Update best R found\n",
    "            R -= step  # Try a smaller R\n",
    "        else:\n",
    "            R += step  # Increase R\n",
    "    return best_R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient k-medians\n",
    "Below: baseline resilient k-medians algorithm\n",
    "- Partittion around medoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_distance_matrix(data):\n",
    "    return np.linalg.norm(data[:, np.newaxis] - data, axis=2)\n",
    "\n",
    "def assign_clusters(data, medoids, distance_matrix):\n",
    "    return np.argmin(distance_matrix[:, medoids], axis=1)\n",
    "\n",
    "def update_medoids(data, clusters, k):\n",
    "    new_medoids = np.zeros(k, dtype=int)\n",
    "    for i in range(k):\n",
    "        cluster_points = data[clusters == i]\n",
    "        if cluster_points.size == 0:\n",
    "            continue\n",
    "        distances = np.sum(np.linalg.norm(cluster_points[:, np.newaxis] - cluster_points, axis=2), axis=1)\n",
    "        new_medoids[i] = cluster_points[np.argmin(distances)]\n",
    "    return new_medoids\n",
    "\n",
    "def PAM(data, k):\n",
    "    n = data.shape[0]\n",
    "    medoids = np.random.choice(n, k, replace=False)\n",
    "    distance_matrix = calculate_distance_matrix(data)\n",
    "    \n",
    "    while True:\n",
    "        clusters = assign_clusters(data, medoids, distance_matrix)\n",
    "        new_medoids = update_medoids(data, clusters, k)\n",
    "    \n",
    "        if np.array_equal(medoids, new_medoids):\n",
    "            break\n",
    "        \n",
    "        medoids = new_medoids\n",
    "\n",
    "    return medoids, clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient k-medoids\n",
    "Below: baseline resilient k-medoids algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_distance_matrix(data):\n",
    "    return np.linalg.norm(data[:, np.newaxis] - data, axis=2)\n",
    "\n",
    "def assign_clusters(data, medoids, distance_matrix):\n",
    "    return np.argmin(distance_matrix[:, medoids], axis=1)\n",
    "\n",
    "def update_medoids(data, clusters, k):\n",
    "    new_medoids = np.zeros(k, dtype=int)\n",
    "    for i in range(k):\n",
    "        cluster_points = data[clusters == i]\n",
    "        if cluster_points.size == 0:\n",
    "            continue\n",
    "        distances = np.sum(np.linalg.norm(cluster_points[:, np.newaxis] - cluster_points, axis=2), axis=1)\n",
    "        new_medoids[i] = cluster_points[np.argmin(distances)]\n",
    "    return new_medoids\n",
    "\n",
    "def PAM(data, k):\n",
    "    n = data.shape[0]\n",
    "    medoids = np.random.choice(n, k, replace=False)\n",
    "    distance_matrix = calculate_distance_matrix(data)\n",
    "    \n",
    "    while True:\n",
    "        clusters = assign_clusters(data, medoids, distance_matrix)\n",
    "        new_medoids = update_medoids(data, clusters, k)\n",
    "    \n",
    "        if np.array_equal(medoids, new_medoids):\n",
    "            break\n",
    "        \n",
    "        medoids = new_medoids\n",
    "\n",
    "    return medoids, clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Clustering Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(data, threshold=0.5):\n",
    "    \"\"\"Create a graph from the data points based on a similarity threshold.\"\"\"\n",
    "    G = nx.Graph()\n",
    "    num_points = data.shape[0]\n",
    "    \n",
    "    for i in range(num_points):\n",
    "        for j in range(i + 1, num_points):\n",
    "            # Compute similarity (using Gaussian kernel)\n",
    "            similarity = np.exp(-np.linalg.norm(data[i] - data[j]) ** 2 / (2 * threshold ** 2))\n",
    "            if similarity > 0:\n",
    "                G.add_edge(i, j, weight=similarity)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def compute_laplacian(G):\n",
    "    \"\"\"Compute the Laplacian matrix of the graph.\"\"\"\n",
    "    L = nx.laplacian_matrix(G).toarray()\n",
    "    return L\n",
    "\n",
    "def graph_cut_clustering(data, num_clusters):\n",
    "    \"\"\"Perform graph cut clustering.\"\"\"\n",
    "    # Step 1: Create the graph\n",
    "    G = create_graph(data)\n",
    "    \n",
    "    # Step 2: Compute the Laplacian matrix\n",
    "    L = compute_laplacian(G)\n",
    "    \n",
    "    # Step 3: Eigenvalue decomposition\n",
    "    eigvals, eigvecs = eigh(L)\n",
    "    \n",
    "    # Step 4: Take the first `num_clusters` eigenvectors\n",
    "    Y = eigvecs[:, :num_clusters]\n",
    "    \n",
    "    # Step 5: Normalize the rows of Y\n",
    "    Y_normalized = Y / np.linalg.norm(Y, axis=1, keepdims=True)\n",
    "    \n",
    "    # Step 6: K-means clustering on the normalized eigenvector matrix\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    clusters = kmeans.fit_predict(Y_normalized)\n",
    "    \n",
    "    return clusters\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_ws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
